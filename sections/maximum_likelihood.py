import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

def display_section():
    """Display the Maximum Likelihood section."""
    
    # Section header with orange diamond
    st.markdown('<div class="orange-diamond">‚óÜ</div><h2 class="section-header">Maximum Likelihood Estimation</h2>', 
                unsafe_allow_html=True)
    
    st.markdown("""
    This section covers maximum likelihood estimation (MLE), a powerful method for estimating
    parameters in statistical models. We'll explore the theory, computation, and applications
    of MLE in econometrics.
    """)
    
    # Create a DataFrame for MLE concepts
    concepts = {
        "Concept": [
            "Likelihood Function",
            "Log-Likelihood",
            "Score Function",
            "Information Matrix",
            "MLE Properties",
            "Numerical Methods",
            "Standard Errors",
            "Hypothesis Tests"
        ],
        "Definition": [
            "L(Œ∏;x) = ‚àè·µ¢f(x·µ¢|Œ∏)",
            "‚Ñì(Œ∏;x) = ‚àë·µ¢log f(x·µ¢|Œ∏)",
            "‚àÇ‚Ñì/‚àÇŒ∏ = 0",
            "-E[‚àÇ¬≤‚Ñì/‚àÇŒ∏¬≤]",
            "Consistent, Efficient",
            "Newton-Raphson, BHHH",
            "‚àödiag(I‚Åª¬π)",
            "Wald, LR, LM tests"
        ],
        "Key Features": [
            "Joint density",
            "Monotonic transformation",
            "First-order condition",
            "Variance of estimator",
            "Asymptotic normality",
            "Iterative solutions",
            "Asymptotic approximation",
            "Asymptotically equivalent"
        ],
        "Applications": [
            "Parameter estimation",
            "Computational convenience",
            "Optimization condition",
            "Efficiency bounds",
            "Inference",
            "Finding maximum",
            "Confidence intervals",
            "Model comparison"
        ]
    }
    
    # Convert to DataFrame and display as table
    df = pd.DataFrame(concepts)
    
    # Use st.markdown to display the table with HTML formatting
    st.markdown(
        df.style.hide(axis="index")
        .to_html()
        .replace('<table', '<table class="concept-table"')
        .replace('<td>', '<td style="text-align: left; padding: 8px;">')
        .replace('<th>', '<th style="text-align: left; background-color: #f2f2f2; padding: 8px;">'),
        unsafe_allow_html=True
    )
    
    # Key Concepts with Tabs
    st.markdown("### üîë Key Concepts")
    
    tabs = st.tabs(["Likelihood Functions", "Numerical Methods", "Hypothesis Testing"])
    
    with tabs[0]:
        st.markdown("#### Likelihood Functions and Properties")
        
        # Create visualization of likelihood function
        np.random.seed(42)
        n = 50
        true_mu = 2
        true_sigma = 1.5
        
        # Generate data
        data = np.random.normal(true_mu, true_sigma, n)
        
        # Create grid for parameters
        mu_grid = np.linspace(0, 4, 100)
        sigma_grid = np.linspace(0.5, 3, 100)
        mu_mesh, sigma_mesh = np.meshgrid(mu_grid, sigma_grid)
        
        # Calculate log-likelihood
        ll = np.zeros_like(mu_mesh)
        for i in range(len(mu_grid)):
            for j in range(len(sigma_grid)):
                ll[j,i] = np.sum(stats.norm.logpdf(data, mu_grid[i], sigma_grid[j]))
        
        # Create plot
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # Contour plot
        cs = ax1.contour(mu_mesh, sigma_mesh, ll, levels=20)
        ax1.clabel(cs, inline=1, fontsize=8)
        ax1.plot(true_mu, true_sigma, 'r*', markersize=15, label='True Value')
        ax1.set_title('Log-Likelihood Contours')
        ax1.set_xlabel('Œº')
        ax1.set_ylabel('œÉ')
        ax1.legend()
        
        # Profile likelihood for Œº
        max_ll_mu = np.max(ll, axis=0)
        ax2.plot(mu_grid, max_ll_mu)
        ax2.axvline(true_mu, color='r', linestyle='--', alpha=0.5)
        ax2.set_title('Profile Likelihood for Œº')
        ax2.set_xlabel('Œº')
        ax2.set_ylabel('Profile Log-Likelihood')
        
        plt.tight_layout()
        st.pyplot(fig)
        
        st.markdown("""
        **Likelihood Function Properties:**
        
        1. **Construction**
           ```python
           L(Œ∏;x) = ‚àè·µ¢f(x·µ¢|Œ∏)
           ‚Ñì(Œ∏;x) = ‚àë·µ¢log f(x·µ¢|Œ∏)
           ```
           - Product of individual densities
           - Log transformation for computation
           - Maintains same maximum
        
        2. **Score Function**
           ```python
           s(Œ∏) = ‚àÇ‚Ñì/‚àÇŒ∏ = ‚àë·µ¢‚àÇlog f(x·µ¢|Œ∏)/‚àÇŒ∏
           ```
           - First derivative of log-likelihood
           - Zero at maximum
           - Information equality
        
        3. **Information Matrix**
           ```python
           I(Œ∏) = -E[‚àÇ¬≤‚Ñì/‚àÇŒ∏¬≤]
           ```
           - Negative expected Hessian
           - Variance of score
           - Efficiency bound
        
        4. **Asymptotic Properties**
           - Consistency: Œ∏ÃÇ ‚Üí·µñ Œ∏‚ÇÄ
           - Normality: ‚àön(Œ∏ÃÇ-Œ∏‚ÇÄ) ‚Üí·µà N(0,I‚Åª¬π)
           - Efficiency: Achieves CR bound
           - Invariance to transformation
        """)
    
    with tabs[1]:
        st.markdown("#### Numerical Methods")
        
        # Create visualization of Newton-Raphson
        def f(x):
            return -0.5 * (x - 2)**2  # Negative quadratic (log-likelihood shape)
        
        def f_prime(x):
            return -(x - 2)  # Derivative
        
        # Create data for visualization
        x = np.linspace(0, 4, 100)
        x_nr = [0.5]  # Starting point
        
        for _ in range(3):
            x_new = x_nr[-1] - f_prime(x_nr[-1])/(-1)  # Newton-Raphson step
            x_nr.append(x_new)
        
        # Create plot
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # Plot function and iterations
        ax1.plot(x, f(x), 'b-', label='Log-Likelihood')
        for i in range(len(x_nr)-1):
            # Plot point
            ax1.plot(x_nr[i], f(x_nr[i]), 'ro')
            # Plot tangent line
            x_tang = np.linspace(x_nr[i]-0.5, x_nr[i]+0.5, 10)
            y_tang = f(x_nr[i]) + f_prime(x_nr[i])*(x_tang - x_nr[i])
            ax1.plot(x_tang, y_tang, 'g--', alpha=0.5)
            # Plot vertical line to next point
            ax1.plot([x_nr[i], x_nr[i+1]], [f(x_nr[i]), f(x_nr[i+1])], 'r:')
        
        ax1.plot(x_nr[-1], f(x_nr[-1]), 'ro')
        ax1.set_title('Newton-Raphson Method')
        ax1.set_xlabel('Parameter')
        ax1.set_ylabel('Log-Likelihood')
        ax1.grid(True, alpha=0.3)
        
        # Convergence plot
        ax2.plot(range(len(x_nr)), np.abs(np.array(x_nr) - 2), 'bo-')
        ax2.set_title('Convergence')
        ax2.set_xlabel('Iteration')
        ax2.set_ylabel('|Œ∏‚Çú - Œ∏*|')
        ax2.set_yscale('log')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        st.pyplot(fig)
        
        st.markdown("""
        **Numerical Optimization Methods:**
        
        1. **Newton-Raphson**
           ```python
           Œ∏‚Çú‚Çä‚ÇÅ = Œ∏‚Çú - [‚àÇ¬≤‚Ñì/‚àÇŒ∏¬≤]‚Åª¬π ¬∑ ‚àÇ‚Ñì/‚àÇŒ∏
           ```
           - Quadratic convergence
           - Requires Hessian
           - Sensitive to starting values
        
        2. **BHHH Algorithm**
           ```python
           Œ∏‚Çú‚Çä‚ÇÅ = Œ∏‚Çú + [‚àë·µ¢s·µ¢s·µ¢']‚Åª¬π ¬∑ ‚àë·µ¢s·µ¢
           ```
           - Uses outer product of scores
           - More stable than Newton-Raphson
           - Linear convergence
        
        3. **Quasi-Newton Methods**
           ```python
           Œ∏‚Çú‚Çä‚ÇÅ = Œ∏‚Çú + Œª‚ÇúH‚Çúg‚Çú
           ```
           - BFGS, DFP updates
           - No exact Hessian needed
           - Superlinear convergence
        
        4. **Implementation Issues**
           - Starting values
           - Step size (line search)
           - Convergence criteria
           - Multiple maxima
        """)
    
    with tabs[2]:
        st.markdown("#### Hypothesis Testing")
        
        # Create visualization of likelihood ratio test
        np.random.seed(42)
        n = 100
        
        # Generate data
        x = np.random.normal(1.5, 1, n)
        
        # Calculate likelihood ratio test statistic
        def ll_restricted(mu):
            return np.sum(stats.norm.logpdf(x, mu, 1))
        
        def ll_unrestricted(mu, sigma):
            return np.sum(stats.norm.logpdf(x, mu, sigma))
        
        # Grid for visualization
        mu_grid = np.linspace(0, 3, 100)
        ll_r = [ll_restricted(mu) for mu in mu_grid]
        
        # Create plot
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # Plot likelihood ratio test
        ax1.plot(mu_grid, ll_r, 'b-', label='Restricted (œÉ=1)')
        ax1.axvline(np.mean(x), color='r', linestyle='--', 
                   label='Unrestricted MLE')
        ax1.axvline(1.5, color='g', linestyle='--',
                   label='True Value')
        ax1.set_title('Likelihood Ratio Test')
        ax1.set_xlabel('Œº')
        ax1.set_ylabel('Log-Likelihood')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Plot chi-square distribution
        x_chi = np.linspace(0, 10, 100)
        ax2.plot(x_chi, stats.chi2.pdf(x_chi, df=1), 'b-')
        ax2.fill_between(x_chi[x_chi >= 3.84], 
                        stats.chi2.pdf(x_chi[x_chi >= 3.84], df=1),
                        alpha=0.3)
        ax2.text(4, 0.1, 'Critical\nValue\n(5%)', ha='left')
        ax2.set_title('œá¬≤ Distribution (df=1)')
        ax2.set_xlabel('Test Statistic')
        ax2.set_ylabel('Density')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        st.pyplot(fig)
        
        st.markdown("""
        **Hypothesis Testing in MLE:**
        
        1. **Likelihood Ratio (LR) Test**
           ```python
           LR = 2[‚Ñì(Œ∏ÃÇ) - ‚Ñì(Œ∏ÃÉ)]
           ```
           - Compare unrestricted vs restricted
           - Asymptotically œá¬≤ distributed
           - Invariant to parameterization
        
        2. **Wald Test**
           ```python
           W = (Œ∏ÃÇ - Œ∏‚ÇÄ)'VÃÇ‚Åª¬π(Œ∏ÃÇ - Œ∏‚ÇÄ)
           ```
           - Uses unrestricted estimates
           - Requires variance matrix
           - Not invariant to transformation
        
        3. **Lagrange Multiplier (LM) Test**
           ```python
           LM = s(Œ∏ÃÉ)'I(Œ∏ÃÉ)‚Åª¬πs(Œ∏ÃÉ)
           ```
           - Uses restricted estimates
           - Score principle
           - Locally most powerful
        
        4. **Testing Strategy**
           - All asymptotically equivalent
           - LR between Wald and LM
           - Choose based on computation
           - Multiple restrictions
        """)
    
    # Application Example
    st.markdown("### üìù Application Example")
    
    st.markdown("""
    **Example: Duration Model Estimation**
    
    Consider modeling unemployment duration:
    
    **Step 1: Model Specification**
    ```python
    # Weibull hazard model
    h(t|x) = ŒªŒ±t^(Œ±-1)exp(xŒ≤)
    
    # Log-likelihood function
    ‚Ñì(Œ∏) = ‚àë·µ¢[d·µ¢log h(t·µ¢|x·µ¢) - H(t·µ¢|x·µ¢)]
    ```
    
    **Step 2: Estimation Results**
    ```python
    Parameters:
    - Œ± (duration dependence): 0.85 (0.05)
    - Œ≤‚ÇÅ (education): -0.12 (0.03)
    - Œ≤‚ÇÇ (age): 0.02 (0.01)
    - Œ≤‚ÇÉ (unemployment rate): 0.15 (0.04)
    
    Log-likelihood: -1234.5
    ```
    
    **Step 3: Hypothesis Tests**
    ```python
    1. No duration dependence (H‚ÇÄ: Œ± = 1)
       - LR stat = 8.5
       - p-value = 0.004
    
    2. Joint significance of covariates
       - Wald œá¬≤(3) = 45.2
       - p-value < 0.001
    ```
    
    **Step 4: Interpretation**
    - Negative duration dependence
    - Education reduces hazard
    - Higher unemployment rate increases duration
    - Model fits well
    """)
    
    # Connection to next section
    st.markdown("### ‚¨áÔ∏è Connection to Simulation Methods")
    
    st.markdown("""
    Maximum likelihood estimation connects to simulation methods through:
    - Simulated maximum likelihood
    - Bootstrap standard errors
    - Monte Carlo studies
    - Numerical integration
    
    The next section will explore these simulation methods.
    """) 